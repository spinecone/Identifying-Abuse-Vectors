# Exercise 2: Automated abuse prevention

Some common ways to address abuse vectors are:

* **Take out the feature** \(you can't abuse what doesn't exist! \[points to head.gif\] \)

* **Reduce interaction** \(users could still interact with this feature, but their speech is filtered or limited in some other way\)

* **Reduce visibility** \(this feature can only be used by certain people or only under certain circumstances\)

* **Don't keep data you don't need** \(this data could be used to hurt people, do we really need to hang on to it?\)

* **Intervene before, during, and after harassment **\(software that either gently encourages people not to be abusive or exacts swift punishment when they do\)

* **Make it opt-in **\(some people are ok with the possibility that they might be harassed, but others aren't\)

* **Add moderation **\(it's too complicated to write a program to prevent abuse of this feature, so let's get some humans to figure it out for us on a case-by-case basis\)

These options can be divided into two categories: tools that **automatically** prevent or penalize harassment, and tools that **empower humans** to do so. For this exercise, we're just going to focus on the **automatic tools**, making anti-harassment robots to do our work for us:

* **Take out the feature**

* **Reduce interaction**

* **Reduce visibility**

* **Don't keep data you don't need**



